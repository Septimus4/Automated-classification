{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8253358b63b7d6b",
   "metadata": {},
   "source": [
    "# Phase 2: Feature Engineering - TechNova Partners Turnover Analysis\n",
    "\n",
    "**Objective**: Transform the cleaned dataset into model-ready features (X) and target (y) for machine learning.\n",
    "\n",
    "**Key Tasks**:\n",
    "- Create engineered features from raw data\n",
    "- Handle categorical variables through encoding\n",
    "- Analyze feature correlations and remove redundant variables\n",
    "- Prepare final modeling dataset\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f8ba810034be13",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2cb6bc063bba02",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T12:57:17.491234Z",
     "start_time": "2025-07-23T12:57:17.446708Z"
    },
    "execution": {
     "iopub.execute_input": "2025-07-23T13:31:19.246890Z",
     "iopub.status.busy": "2025-07-23T13:31:19.246487Z",
     "iopub.status.idle": "2025-07-23T13:31:21.341601Z",
     "shell.execute_reply": "2025-07-23T13:31:21.340986Z"
    }
   },
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Feature engineering libraries\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# System libraries\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Environment setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bfbb377638ac6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T12:57:17.682632Z",
     "start_time": "2025-07-23T12:57:17.530900Z"
    },
    "execution": {
     "iopub.execute_input": "2025-07-23T13:31:21.345748Z",
     "iopub.status.busy": "2025-07-23T13:31:21.345374Z",
     "iopub.status.idle": "2025-07-23T13:31:22.012749Z",
     "shell.execute_reply": "2025-07-23T13:31:22.011243Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setup robust path handling and load data\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to Python path and setup environment\n",
    "current_dir = Path.cwd()\n",
    "project_root = current_dir\n",
    "while project_root != project_root.parent:\n",
    "    if (project_root / 'pyproject.toml').exists() or (project_root / 'hr_analytics_utils.py').exists():\n",
    "        break\n",
    "    project_root = project_root.parent\n",
    "\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import utilities and setup environment\n",
    "from hr_analytics_utils import setup_notebook_environment, load_cleaned_data_from_db\n",
    "\n",
    "# Setup environment\n",
    "env_info = setup_notebook_environment()\n",
    "\n",
    "# Load the processed data using robust database path\n",
    "try:\n",
    "    df = load_cleaned_data_from_db()\n",
    "    print(f\"Data loaded from database: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Database file not found. Please ensure notebook 1 (data wrangling) has been executed first.\")\n",
    "    raise\n",
    "\n",
    "print(f\"Columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56abb9f32f50304c",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddf9e67e5853052",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T12:57:17.704160Z",
     "start_time": "2025-07-23T12:57:17.693125Z"
    },
    "execution": {
     "iopub.execute_input": "2025-07-23T13:31:22.017649Z",
     "iopub.status.busy": "2025-07-23T13:31:22.017250Z",
     "iopub.status.idle": "2025-07-23T13:31:22.065326Z",
     "shell.execute_reply": "2025-07-23T13:31:22.062898Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create engineered features\n",
    "print(\"Creating engineered features:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# 1. Career progression features\n",
    "df['years_in_position_ratio'] = df['annees_dans_le_poste_actuel'] / df['annees_dans_l_entreprise']\n",
    "df['years_in_position_ratio'] = df['years_in_position_ratio'].fillna(0)\n",
    "\n",
    "# 2. Experience features\n",
    "df['experience_before_company'] = df['annee_experience_totale'] - df['annees_dans_l_entreprise']\n",
    "df['experience_before_company'] = np.maximum(df['experience_before_company'], 0)\n",
    "\n",
    "# 3. Age-related features\n",
    "df['age_group'] = pd.cut(df['age'], bins=[0, 30, 40, 50, 100], labels=['<30', '30-40', '40-50', '50+'])\n",
    "df['career_stage'] = pd.cut(df['annee_experience_totale'], bins=[0, 3, 8, 15, 100], labels=['Junior', 'Mid', 'Senior', 'Expert'])\n",
    "\n",
    "# 4. Compensation features\n",
    "df['salary_quartile'] = pd.qcut(df['revenu_mensuel'], q=4, labels=['Q1', 'Q2', 'Q3', 'Q4'])\n",
    "df['salary_per_experience'] = df['revenu_mensuel'] / (df['annee_experience_totale'] + 1)\n",
    "\n",
    "# 5. Tenure features\n",
    "df['tenure_group'] = pd.cut(df['annees_dans_l_entreprise'], bins=[0, 1, 3, 7, 100], labels=['New', 'Early', 'Mid', 'Long'])\n",
    "df['recent_hire'] = (df['annees_dans_l_entreprise'] <= 1).astype(int)\n",
    "\n",
    "# 6. Performance features (if available)\n",
    "if 'note_evaluation_actuelle' in df.columns and 'note_evaluation_precedente' in df.columns:\n",
    "    df['performance_trend'] = df['note_evaluation_actuelle'] - df['note_evaluation_precedente']\n",
    "    df['performance_stable'] = (df['note_evaluation_actuelle'] == df['note_evaluation_precedente']).astype(int)\n",
    "\n",
    "# 7. Satisfaction composite scores\n",
    "satisfaction_cols = [col for col in df.columns if 'satisfaction' in col.lower()]\n",
    "if satisfaction_cols:\n",
    "    df['satisfaction_mean'] = df[satisfaction_cols].mean(axis=1)\n",
    "    df['satisfaction_std'] = df[satisfaction_cols].std(axis=1)\n",
    "    df['low_satisfaction'] = (df['satisfaction_mean'] <= 2).astype(int)\n",
    "\n",
    "# 8. Work-life balance features\n",
    "if 'distance_domicile_travail' in df.columns:\n",
    "    df['long_commute'] = (df['distance_domicile_travail'] > 20).astype(int)\n",
    "\n",
    "# 9. Development features\n",
    "if 'nb_formations_suivies' in df.columns:\n",
    "    df['training_per_year'] = df['nb_formations_suivies'] / (df['annees_dans_l_entreprise'] + 1)\n",
    "    df['no_training'] = (df['nb_formations_suivies'] == 0).astype(int)\n",
    "\n",
    "# 10. Promotion features\n",
    "if 'annees_depuis_la_derniere_promotion' in df.columns:\n",
    "    df['overdue_promotion'] = (df['annees_depuis_la_derniere_promotion'] > 3).astype(int)\n",
    "    df['never_promoted'] = (df['annees_depuis_la_derniere_promotion'] == df['annees_dans_l_entreprise']).astype(int)\n",
    "\n",
    "print(f\"Feature engineering complete. New shape: {df.shape}\")\n",
    "print(f\" New engineered features: {len(df.columns) - len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb85c5f612df56a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T12:57:17.748680Z",
     "start_time": "2025-07-23T12:57:17.741215Z"
    },
    "execution": {
     "iopub.execute_input": "2025-07-23T13:31:22.069660Z",
     "iopub.status.busy": "2025-07-23T13:31:22.069422Z",
     "iopub.status.idle": "2025-07-23T13:31:22.093374Z",
     "shell.execute_reply": "2025-07-23T13:31:22.092819Z"
    }
   },
   "outputs": [],
   "source": [
    "# Handle categorical variables\n",
    "print(\"Handling categorical variables:\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Identify categorical columns\n",
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "print(f\"Categorical columns: {categorical_cols}\")\n",
    "\n",
    "# Remove target and ID columns from categorical list\n",
    "categorical_cols = [col for col in categorical_cols if col not in ['a_quitte_l_entreprise', 'target_turnover', 'employee_id']]\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "df_encoded = pd.get_dummies(df, columns=categorical_cols, prefix=categorical_cols, drop_first=True)\n",
    "\n",
    "print(f\"Categorical encoding complete. New shape: {df_encoded.shape}\")\n",
    "print(f\" Total features after encoding: {len(df_encoded.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04d100cdf3b8237",
   "metadata": {},
   "source": [
    "## 3. Correlation Analysis & Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3253b37c53907e2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T12:57:17.906951Z",
     "start_time": "2025-07-23T12:57:17.791069Z"
    },
    "execution": {
     "iopub.execute_input": "2025-07-23T13:31:22.098115Z",
     "iopub.status.busy": "2025-07-23T13:31:22.097838Z",
     "iopub.status.idle": "2025-07-23T13:31:22.462450Z",
     "shell.execute_reply": "2025-07-23T13:31:22.461526Z"
    }
   },
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "print(\"Correlation Analysis:\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "# Select only numerical columns for correlation analysis\n",
    "numerical_cols = df_encoded.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numerical_cols = [col for col in numerical_cols if col not in ['employee_id', 'id_employee']]\n",
    "\n",
    "print(f\"Analyzing {len(numerical_cols)} numerical features\")\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = df_encoded[numerical_cols].corr()\n",
    "\n",
    "# Find highly correlated features (correlation > 0.8)\n",
    "high_corr_pairs = []\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        if abs(correlation_matrix.iloc[i, j]) > 0.8:\n",
    "            high_corr_pairs.append((\n",
    "                correlation_matrix.columns[i], \n",
    "                correlation_matrix.columns[j], \n",
    "                correlation_matrix.iloc[i, j]\n",
    "            ))\n",
    "\n",
    "print(f\"\\nHighly correlated pairs (|r| > 0.8): {len(high_corr_pairs)}\")\n",
    "for pair in high_corr_pairs[:10]:  # Show first 10\n",
    "    print(f\"  {pair[0]} - {pair[1]}: {pair[2]:.3f}\")\n",
    "\n",
    "# Plot correlation with target variable\n",
    "target_correlations = correlation_matrix['target_turnover'].sort_values(key=abs, ascending=False)\n",
    "print(\"\\nTop 15 correlations with turnover:\")\n",
    "print(target_correlations.head(15))\n",
    "\n",
    "# Visualize top correlations\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_corr = target_correlations.head(15)\n",
    "colors = ['red' if x > 0 else 'blue' for x in top_corr.values]\n",
    "plt.barh(range(len(top_corr)), top_corr.values, color=colors)\n",
    "plt.yticks(range(len(top_corr)), top_corr.index)\n",
    "plt.xlabel('Correlation with Turnover')\n",
    "plt.title('Top 15 Feature Correlations with Turnover')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520b878a1031d3b5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T12:57:17.964264Z",
     "start_time": "2025-07-23T12:57:17.961330Z"
    },
    "execution": {
     "iopub.execute_input": "2025-07-23T13:31:22.465506Z",
     "iopub.status.busy": "2025-07-23T13:31:22.465252Z",
     "iopub.status.idle": "2025-07-23T13:31:22.476238Z",
     "shell.execute_reply": "2025-07-23T13:31:22.475712Z"
    }
   },
   "outputs": [],
   "source": [
    "# Feature selection - remove highly correlated features\n",
    "print(\"Feature Selection:\")\n",
    "print(\"=\" * 20)\n",
    "\n",
    "# Remove features with high correlation (keep the one with higher correlation to target)\n",
    "features_to_remove = set()\n",
    "\n",
    "for pair in high_corr_pairs:\n",
    "    feat1, feat2, corr = pair\n",
    "    if feat1 in numerical_cols and feat2 in numerical_cols:\n",
    "        # Keep the feature with higher absolute correlation to target\n",
    "        corr1 = abs(correlation_matrix.loc[feat1, 'target_turnover'])\n",
    "        corr2 = abs(correlation_matrix.loc[feat2, 'target_turnover'])\n",
    "        \n",
    "        if corr1 > corr2:\n",
    "            features_to_remove.add(feat2)\n",
    "        else:\n",
    "            features_to_remove.add(feat1)\n",
    "\n",
    "print(f\"Features to remove due to high correlation: {len(features_to_remove)}\")\n",
    "print(f\"Features: {list(features_to_remove)[:10]}...\")  # Show first 10\n",
    "\n",
    "# Remove highly correlated features\n",
    "df_selected = df_encoded.drop(columns=features_to_remove)\n",
    "print(f\"\\nFeature selection complete. Shape: {df_selected.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da53a55cd524cc90",
   "metadata": {},
   "source": [
    "## 4. Final Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68206176f58c1c1f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T12:57:18.012220Z",
     "start_time": "2025-07-23T12:57:18.006721Z"
    },
    "execution": {
     "iopub.execute_input": "2025-07-23T13:31:22.478926Z",
     "iopub.status.busy": "2025-07-23T13:31:22.478682Z",
     "iopub.status.idle": "2025-07-23T13:31:22.508299Z",
     "shell.execute_reply": "2025-07-23T13:31:22.507739Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare final X and y\n",
    "print(\"Preparing final dataset:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Define target variable\n",
    "y = df_selected['target_turnover']\n",
    "\n",
    "# Define features (remove target and non-feature columns)\n",
    "columns_to_exclude = [\n",
    "    'target_turnover', 'a_quitte_l_entreprise', 'employee_id', 'id_employee', \n",
    "    'eval_number', 'code_sondage'\n",
    "]\n",
    "\n",
    "X = df_selected.drop(columns=[col for col in columns_to_exclude if col in df_selected.columns])\n",
    "\n",
    "print(f\"Final dataset prepared:\")\n",
    "print(f\"   Features (X): {X.shape}\")\n",
    "print(f\"   Target (y): {y.shape}\")\n",
    "print(f\"   Target distribution: {y.value_counts()}\")\n",
    "print(f\"   Turnover rate: {y.mean():.2%}\")\n",
    "\n",
    "# Check for any remaining missing values\n",
    "missing_values = X.isnull().sum().sum()\n",
    "print(f\"   Missing values in X: {missing_values}\")\n",
    "\n",
    "# Handle any remaining missing values\n",
    "if missing_values > 0:\n",
    "    print(\"\\nHandling missing values:\")\n",
    "    # Fill numerical missing values with median\n",
    "    numerical_cols = X.select_dtypes(include=[np.number]).columns\n",
    "    X[numerical_cols] = X[numerical_cols].fillna(X[numerical_cols].median())\n",
    "    \n",
    "    # Fill categorical missing values with mode\n",
    "    categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_cols:\n",
    "        X[col] = X[col].fillna(X[col].mode()[0])\n",
    "    \n",
    "    print(f\"   Missing values after handling: {X.isnull().sum().sum()}\")\n",
    "\n",
    "print(f\"\\n Feature types:\")\n",
    "print(f\"   Numerical: {len(X.select_dtypes(include=[np.number]).columns)}\")\n",
    "print(f\"   Categorical: {len(X.select_dtypes(include=['object']).columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1caaab75e5262c3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T12:57:18.143606Z",
     "start_time": "2025-07-23T12:57:18.060977Z"
    },
    "execution": {
     "iopub.execute_input": "2025-07-23T13:31:22.511263Z",
     "iopub.status.busy": "2025-07-23T13:31:22.511029Z",
     "iopub.status.idle": "2025-07-23T13:31:22.799918Z",
     "shell.execute_reply": "2025-07-23T13:31:22.799362Z"
    }
   },
   "outputs": [],
   "source": [
    "# Quick feature importance preview using correlation\n",
    "print(\"Feature Importance Preview:\")\n",
    "print(\"=\" * 32)\n",
    "\n",
    "# Calculate correlations for numerical features\n",
    "numerical_features = X.select_dtypes(include=[np.number]).columns\n",
    "correlations = {}\n",
    "\n",
    "for col in numerical_features:\n",
    "    if col in X.columns:\n",
    "        correlation = abs(X[col].corr(y))\n",
    "        correlations[col] = correlation\n",
    "\n",
    "# Sort by correlation strength\n",
    "sorted_correlations = sorted(correlations.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Top 20 features by correlation with turnover:\")\n",
    "for i, (feature, correlation) in enumerate(sorted_correlations[:20]):\n",
    "    print(f\"{i+1:2d}. {feature:<30} {correlation:.4f}\")\n",
    "\n",
    "# Visualize top features\n",
    "top_features = [item[0] for item in sorted_correlations[:15]]\n",
    "top_correlations = [item[1] for item in sorted_correlations[:15]]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.barh(range(len(top_features)), top_correlations)\n",
    "plt.yticks(range(len(top_features)), top_features)\n",
    "plt.xlabel('Absolute Correlation with Turnover')\n",
    "plt.title('Top 15 Features by Correlation with Turnover')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70353f7ec1830dc",
   "metadata": {},
   "source": [
    "## 5. Export Prepared Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc154a69ad2d996",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T12:57:19.989164Z",
     "start_time": "2025-07-23T12:57:18.203216Z"
    },
    "execution": {
     "iopub.execute_input": "2025-07-23T13:31:22.803171Z",
     "iopub.status.busy": "2025-07-23T13:31:22.802897Z",
     "iopub.status.idle": "2025-07-23T13:31:32.614782Z",
     "shell.execute_reply": "2025-07-23T13:31:32.614253Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save to SQLite\n",
    "from sqlalchemy import create_engine\n",
    "output_path = Path('../results')\n",
    "\n",
    "engine = create_engine('sqlite:///' + str(output_path / 'technova_hr.db'))\n",
    "df_selected.to_sql('modeling_data', engine, index=False, if_exists='replace')\n",
    "X.to_sql('features', engine, index=False, if_exists='replace')\n",
    "y.to_sql('target', engine, index=False, if_exists='replace')\n",
    "\n",
    "# Save feature list\n",
    "feature_info = pd.DataFrame({\n",
    "    'feature_name': X.columns,\n",
    "    'feature_type': [X[col].dtype for col in X.columns],\n",
    "    'correlation_with_target': [abs(X[col].corr(y)) if X[col].dtype in ['int64', 'float64'] else np.nan for col in X.columns]\n",
    "})\n",
    "\n",
    "print(f\"\\nReady for modeling phase!\")\n",
    "print(f\"   Total features: {len(X.columns)}\")\n",
    "print(f\"   Target balance: {y.value_counts().to_dict()}\")\n",
    "print(f\"   Turnover rate: {y.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cbd046ec3c5b0c4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T12:57:22.119351Z",
     "start_time": "2025-07-23T12:57:20.051612Z"
    },
    "execution": {
     "iopub.execute_input": "2025-07-23T13:31:32.619504Z",
     "iopub.status.busy": "2025-07-23T13:31:32.619243Z",
     "iopub.status.idle": "2025-07-23T13:31:43.962309Z",
     "shell.execute_reply": "2025-07-23T13:31:43.959604Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save data to database for subsequent phases\n",
    "from sqlalchemy import create_engine\n",
    "from hr_analytics_utils import get_results_dir, get_database_path\n",
    "\n",
    "# Use robust paths from utilities\n",
    "output_path = get_results_dir()\n",
    "db_path = get_database_path()\n",
    "\n",
    "output_path.mkdir(exist_ok=True)\n",
    "\n",
    "# Create database connection\n",
    "engine = create_engine(f'sqlite:///{db_path}')\n",
    "\n",
    "print(f\"Saving data to database:\")\n",
    "print(f\"   Database path: {db_path}\")\n",
    "print(f\"   Results directory: {output_path}\")\n",
    "\n",
    "# Save features and target to database\n",
    "print(\"Saving to database tables:\")\n",
    "X.to_sql('features_X', engine, index=False, if_exists='replace')\n",
    "y.to_sql('target_y', engine, index=False, if_exists='replace')\n",
    "\n",
    "# Combined dataset for later use\n",
    "df_combined = pd.concat([X, y], axis=1)\n",
    "df_combined.to_sql('modeling_data', engine, index=False, if_exists='replace')\n",
    "\n",
    "print(f\"Data saved to database successfully!\")\n",
    "print(f\"   Features table: {X.shape}\")\n",
    "print(f\"   Target table: {y.shape}\")\n",
    "print(f\"   Combined modeling data: {df_combined.shape}\")\n",
    "\n",
    "# Verify the save\n",
    "try:\n",
    "    test_X = pd.read_sql('features_X', engine)\n",
    "    test_y = pd.read_sql('target_y', engine)\n",
    "    print(f\"Verification successful - Data integrity maintained\")\n",
    "    print(f\"   Features restored: {test_X.shape}\")\n",
    "    print(f\"   Target restored: {test_y.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Verification failed: {e}\")\n",
    "\n",
    "print(f\"\\nAll data saved to: {db_path}\")\n",
    "print(f\"Ready for Phase 3: Baseline Modeling!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automated-classification-dNv-zZCP-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
