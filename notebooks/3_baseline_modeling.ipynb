{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57f30552",
   "metadata": {},
   "source": [
    "# Phase 3: Baseline Modeling - TechNova Partners Turnover Analysis\n",
    "\n",
    "**Objective**: Establish baseline model performance using simple train/test split and fundamental algorithms.\n",
    "\n",
    "**Models to evaluate**:\n",
    "1. **Dummy Classifier** (baseline)\n",
    "2. **Logistic Regression** (linear model)\n",
    "3. **Random Forest** (non-linear tree model)\n",
    "\n",
    "**Metrics**: Confusion matrix, precision, recall, F1-score, classification report\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f218378",
   "metadata": {},
   "source": [
    "## 1. Environment Setup & Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8dd2d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T12:57:34.257343Z",
     "start_time": "2025-07-23T12:57:34.197515Z"
    },
    "execution": {
     "iopub.execute_input": "2025-07-23T13:32:12.643834Z",
     "iopub.status.busy": "2025-07-23T13:32:12.643613Z",
     "iopub.status.idle": "2025-07-23T13:32:14.683078Z",
     "shell.execute_reply": "2025-07-23T13:32:14.682433Z"
    }
   },
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    ")\n",
    "\n",
    "# System libraries\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Environment setup complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d735630",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T12:57:34.796932Z",
     "start_time": "2025-07-23T12:57:34.296250Z"
    },
    "execution": {
     "iopub.execute_input": "2025-07-23T13:32:14.686062Z",
     "iopub.status.busy": "2025-07-23T13:32:14.685675Z",
     "iopub.status.idle": "2025-07-23T13:32:16.723794Z",
     "shell.execute_reply": "2025-07-23T13:32:16.723287Z"
    }
   },
   "outputs": [],
   "source": [
    "# Setup robust path handling and load data\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project root to Python path and setup environment\n",
    "current_dir = Path.cwd()\n",
    "project_root = current_dir\n",
    "while project_root != project_root.parent:\n",
    "    if (project_root / 'pyproject.toml').exists() or (project_root / 'hr_analytics_utils.py').exists():\n",
    "        break\n",
    "    project_root = project_root.parent\n",
    "\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import utilities and setup environment\n",
    "from hr_analytics_utils import setup_notebook_environment, load_modeling_data_from_db, print_database_status\n",
    "\n",
    "# Setup environment\n",
    "env_info = setup_notebook_environment()\n",
    "\n",
    "# Check database status first\n",
    "print_database_status()\n",
    "\n",
    "# Load features and target from database using robust paths\n",
    "X, y = load_modeling_data_from_db()\n",
    "\n",
    "if X is None or y is None:\n",
    "    raise ValueError(\"Could not load data from database. Please ensure notebook 2 has been executed.\")\n",
    "\n",
    "print(f\"\\nData loaded successfully:\")\n",
    "print(f\"   Features shape: {X.shape}\")\n",
    "print(f\"   Target shape: {y.shape}\")\n",
    "print(f\"   Target distribution: {y.value_counts().to_dict()}\")\n",
    "\n",
    "# Check feature types\n",
    "numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"\\nFeature types:\")\n",
    "print(f\"   Numeric features: {len(numeric_features)}\")\n",
    "print(f\"   Categorical features: {len(categorical_features)}\")\n",
    "\n",
    "# Check for missing values\n",
    "print(f\"\\nData quality check:\")\n",
    "print(f\"   Missing values in X: {X.isnull().sum().sum()}\")\n",
    "print(f\"   Missing values in y: {y.isnull().sum()}\")\n",
    "print(f\"   Duplicate rows: {X.duplicated().sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca507b80",
   "metadata": {},
   "source": [
    "## 2. Data Splitting & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f14a56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T12:57:34.815043Z",
     "start_time": "2025-07-23T12:57:34.806150Z"
    },
    "execution": {
     "iopub.execute_input": "2025-07-23T13:32:16.728195Z",
     "iopub.status.busy": "2025-07-23T13:32:16.727777Z",
     "iopub.status.idle": "2025-07-23T13:32:16.747238Z",
     "shell.execute_reply": "2025-07-23T13:32:16.746617Z"
    }
   },
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "print(\" Splitting data into train/test sets:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Stratified split to maintain class balance\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Data split completed:\")\n",
    "print(f\"   Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"   Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"   Features: {X_train.shape[1]}\")\n",
    "\n",
    "# Check class distribution in both sets\n",
    "print(f\"\\nClass distribution:\")\n",
    "print(f\"   Training - Stay: {(y_train == 0).sum()}, Leave: {(y_train == 1).sum()}\")\n",
    "print(f\"   Test - Stay: {(y_test == 0).sum()}, Leave: {(y_test == 1).sum()}\")\n",
    "print(f\"   Training turnover rate: {y_train.mean():.2%}\")\n",
    "print(f\"   Test turnover rate: {y_test.mean():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30d22ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T12:57:34.862572Z",
     "start_time": "2025-07-23T12:57:34.857227Z"
    },
    "execution": {
     "iopub.execute_input": "2025-07-23T13:32:16.750101Z",
     "iopub.status.busy": "2025-07-23T13:32:16.749849Z",
     "iopub.status.idle": "2025-07-23T13:32:16.766338Z",
     "shell.execute_reply": "2025-07-23T13:32:16.765840Z"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocessing - handle categorical variables and scaling\n",
    "print(\"Preprocessing data:\")\n",
    "print(\"=\" * 20)\n",
    "\n",
    "# Separate numerical and categorical features\n",
    "numerical_features = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"Numerical features: {len(numerical_features)}\")\n",
    "print(f\"Categorical features: {len(categorical_features)}\")\n",
    "\n",
    "# Handle categorical variables with one-hot encoding if any exist\n",
    "if categorical_features:\n",
    "    print(\"\\nüìù Encoding categorical variables...\")\n",
    "    X_train_encoded = pd.get_dummies(X_train, columns=categorical_features, drop_first=True)\n",
    "    X_test_encoded = pd.get_dummies(X_test, columns=categorical_features, drop_first=True)\n",
    "    \n",
    "    # Align columns between train and test sets\n",
    "    X_train_encoded, X_test_encoded = X_train_encoded.align(X_test_encoded, join='left', axis=1, fill_value=0)\n",
    "else:\n",
    "    X_train_encoded = X_train.copy()\n",
    "    X_test_encoded = X_test.copy()\n",
    "\n",
    "print(f\"\\nPreprocessing complete:\")\n",
    "print(f\"   Final feature count: {X_train_encoded.shape[1]}\")\n",
    "print(f\"   Training set shape: {X_train_encoded.shape}\")\n",
    "print(f\"   Test set shape: {X_test_encoded.shape}\")\n",
    "\n",
    "# Store processed data\n",
    "X_train_final = X_train_encoded\n",
    "X_test_final = X_test_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3813f0ea",
   "metadata": {},
   "source": [
    "## 3. Baseline Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a319e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T12:57:34.919566Z",
     "start_time": "2025-07-23T12:57:34.907417Z"
    },
    "execution": {
     "iopub.execute_input": "2025-07-23T13:32:16.768841Z",
     "iopub.status.busy": "2025-07-23T13:32:16.768609Z",
     "iopub.status.idle": "2025-07-23T13:32:16.812000Z",
     "shell.execute_reply": "2025-07-23T13:32:16.811473Z"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Dummy Classifier (Baseline)\n",
    "print(\"Dummy Classifier (Baseline):\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Create dummy classifier with different strategies\n",
    "dummy_strategies = ['most_frequent', 'prior', 'stratified']\n",
    "dummy_results = {}\n",
    "\n",
    "for strategy in dummy_strategies:\n",
    "    dummy_clf = DummyClassifier(strategy=strategy, random_state=42)\n",
    "    dummy_clf.fit(X_train_final, y_train)\n",
    "    dummy_pred = dummy_clf.predict(X_test_final)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    precision = precision_score(y_test, dummy_pred, average='binary')\n",
    "    recall = recall_score(y_test, dummy_pred, average='binary')\n",
    "    f1 = f1_score(y_test, dummy_pred, average='binary')\n",
    "    \n",
    "    dummy_results[strategy] = {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{strategy.capitalize()} strategy:\")\n",
    "    print(f\"   Precision: {precision:.4f}\")\n",
    "    print(f\"   Recall: {recall:.4f}\")\n",
    "    print(f\"   F1-score: {f1:.4f}\")\n",
    "\n",
    "# Use the best dummy classifier as baseline\n",
    "best_dummy_strategy = max(dummy_results, key=lambda x: dummy_results[x]['f1'])\n",
    "dummy_baseline = DummyClassifier(strategy=best_dummy_strategy, random_state=42)\n",
    "dummy_baseline.fit(X_train_final, y_train)\n",
    "dummy_pred = dummy_baseline.predict(X_test_final)\n",
    "\n",
    "print(f\"\\nBest dummy strategy: {best_dummy_strategy}\")\n",
    "print(f\"   This will be our baseline to beat.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b047a9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T12:57:35.097165Z",
     "start_time": "2025-07-23T12:57:34.956085Z"
    },
    "execution": {
     "iopub.execute_input": "2025-07-23T13:32:16.815450Z",
     "iopub.status.busy": "2025-07-23T13:32:16.814571Z",
     "iopub.status.idle": "2025-07-23T13:32:17.208051Z",
     "shell.execute_reply": "2025-07-23T13:32:17.207417Z"
    }
   },
   "outputs": [],
   "source": [
    "# 2. Logistic Regression (Linear Model)\n",
    "print(\"Logistic Regression:\")\n",
    "print(\"=\" * 25)\n",
    "\n",
    "# Scale features for logistic regression\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_final)\n",
    "X_test_scaled = scaler.transform(X_test_final)\n",
    "\n",
    "# Create and train logistic regression model\n",
    "log_reg = LogisticRegression(random_state=42, max_iter=1000)\n",
    "log_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "log_reg_pred = log_reg.predict(X_test_scaled)\n",
    "log_reg_pred_proba = log_reg.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "log_reg_precision = precision_score(y_test, log_reg_pred)\n",
    "log_reg_recall = recall_score(y_test, log_reg_pred)\n",
    "log_reg_f1 = f1_score(y_test, log_reg_pred)\n",
    "log_reg_auc = roc_auc_score(y_test, log_reg_pred_proba)\n",
    "\n",
    "print(f\"\\nLogistic Regression Results:\")\n",
    "print(f\"   Precision: {log_reg_precision:.4f}\")\n",
    "print(f\"   Recall: {log_reg_recall:.4f}\")\n",
    "print(f\"   F1-score: {log_reg_f1:.4f}\")\n",
    "print(f\"   AUC-ROC: {log_reg_auc:.4f}\")\n",
    "\n",
    "# Feature importance (coefficients)\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train_final.columns,\n",
    "    'coefficient': log_reg.coef_[0],\n",
    "    'abs_coefficient': np.abs(log_reg.coef_[0])\n",
    "}).sort_values('abs_coefficient', ascending=False)\n",
    "\n",
    "print(f\"\\nTop 10 most important features (by coefficient magnitude):\")\n",
    "for i, row in feature_importance.head(10).iterrows():\n",
    "    print(f\"   {row['feature']:<30} {row['coefficient']:>8.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3e0012",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T12:57:35.365791Z",
     "start_time": "2025-07-23T12:57:35.165992Z"
    },
    "execution": {
     "iopub.execute_input": "2025-07-23T13:32:17.211591Z",
     "iopub.status.busy": "2025-07-23T13:32:17.210443Z",
     "iopub.status.idle": "2025-07-23T13:32:17.907975Z",
     "shell.execute_reply": "2025-07-23T13:32:17.907403Z"
    }
   },
   "outputs": [],
   "source": [
    "# 3. Random Forest (Non-linear Tree Model)\n",
    "print(\"Random Forest:\")\n",
    "print(\"=\" * 17)\n",
    "\n",
    "# Create and train random forest model\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train_final, y_train)\n",
    "\n",
    "# Make predictions\n",
    "rf_pred = rf.predict(X_test_final)\n",
    "rf_pred_proba = rf.predict_proba(X_test_final)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "rf_precision = precision_score(y_test, rf_pred)\n",
    "rf_recall = recall_score(y_test, rf_pred)\n",
    "rf_f1 = f1_score(y_test, rf_pred)\n",
    "rf_auc = roc_auc_score(y_test, rf_pred_proba)\n",
    "\n",
    "print(f\"\\nRandom Forest Results:\")\n",
    "print(f\"   Precision: {rf_precision:.4f}\")\n",
    "print(f\"   Recall: {rf_recall:.4f}\")\n",
    "print(f\"   F1-score: {rf_f1:.4f}\")\n",
    "print(f\"   AUC-ROC: {rf_auc:.4f}\")\n",
    "\n",
    "# Feature importance\n",
    "rf_feature_importance = pd.DataFrame({\n",
    "    'feature': X_train_final.columns,\n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(f\"\\nTop 10 most important features (by Random Forest importance):\")\n",
    "for i, row in rf_feature_importance.head(10).iterrows():\n",
    "    print(f\"   {row['feature']:<30} {row['importance']:>8.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36f979b",
   "metadata": {},
   "source": [
    "## 4. Model Comparison & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e3f4cd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T12:57:35.562179Z",
     "start_time": "2025-07-23T12:57:35.386624Z"
    },
    "execution": {
     "iopub.execute_input": "2025-07-23T13:32:17.911676Z",
     "iopub.status.busy": "2025-07-23T13:32:17.910735Z",
     "iopub.status.idle": "2025-07-23T13:32:18.573717Z",
     "shell.execute_reply": "2025-07-23T13:32:18.573166Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "print(\" Model Comparison:\")\n",
    "print(\"=\" * 20)\n",
    "\n",
    "# Create comparison dataframe\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': ['Dummy (Baseline)', 'Logistic Regression', 'Random Forest'],\n",
    "    'Precision': [\n",
    "        dummy_results[best_dummy_strategy]['precision'],\n",
    "        log_reg_precision,\n",
    "        rf_precision\n",
    "    ],\n",
    "    'Recall': [\n",
    "        dummy_results[best_dummy_strategy]['recall'],\n",
    "        log_reg_recall,\n",
    "        rf_recall\n",
    "    ],\n",
    "    'F1-Score': [\n",
    "        dummy_results[best_dummy_strategy]['f1'],\n",
    "        log_reg_f1,\n",
    "        rf_f1\n",
    "    ],\n",
    "    'AUC-ROC': [\n",
    "        0.5,  # Dummy classifier AUC is 0.5\n",
    "        log_reg_auc,\n",
    "        rf_auc\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(results_df.to_string(index=False, float_format='%.4f'))\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "metrics = ['Precision', 'Recall', 'F1-Score', 'AUC-ROC']\n",
    "for i, metric in enumerate(metrics):\n",
    "    ax = axes[i//2, i%2]\n",
    "    bars = ax.bar(results_df['Model'], results_df[metric])\n",
    "    ax.set_title(f'{metric} Comparison')\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.set_ylim(0, 1)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, value in zip(bars, results_df[metric]):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{value:.3f}', ha='center', va='bottom')\n",
    "    \n",
    "    # Rotate x-axis labels for better readability\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90826e2e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T12:57:35.770372Z",
     "start_time": "2025-07-23T12:57:35.590003Z"
    },
    "execution": {
     "iopub.execute_input": "2025-07-23T13:32:18.576321Z",
     "iopub.status.busy": "2025-07-23T13:32:18.576096Z",
     "iopub.status.idle": "2025-07-23T13:32:19.311582Z",
     "shell.execute_reply": "2025-07-23T13:32:19.311042Z"
    }
   },
   "outputs": [],
   "source": [
    "# Confusion matrices for all models\n",
    "print(\"Confusion Matrices:\")\n",
    "print(\"=\" * 22)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# Dummy classifier confusion matrix\n",
    "dummy_cm = confusion_matrix(y_test, dummy_pred)\n",
    "sns.heatmap(dummy_cm, annot=True, fmt='d', cmap='Blues', ax=axes[0])\n",
    "axes[0].set_title('Dummy Classifier')\n",
    "axes[0].set_xlabel('Predicted')\n",
    "axes[0].set_ylabel('Actual')\n",
    "axes[0].set_xticklabels(['Stay', 'Leave'])\n",
    "axes[0].set_yticklabels(['Stay', 'Leave'])\n",
    "\n",
    "# Logistic regression confusion matrix\n",
    "log_reg_cm = confusion_matrix(y_test, log_reg_pred)\n",
    "sns.heatmap(log_reg_cm, annot=True, fmt='d', cmap='Blues', ax=axes[1])\n",
    "axes[1].set_title('Logistic Regression')\n",
    "axes[1].set_xlabel('Predicted')\n",
    "axes[1].set_ylabel('Actual')\n",
    "axes[1].set_xticklabels(['Stay', 'Leave'])\n",
    "axes[1].set_yticklabels(['Stay', 'Leave'])\n",
    "\n",
    "# Random forest confusion matrix\n",
    "rf_cm = confusion_matrix(y_test, rf_pred)\n",
    "sns.heatmap(rf_cm, annot=True, fmt='d', cmap='Blues', ax=axes[2])\n",
    "axes[2].set_title('Random Forest')\n",
    "axes[2].set_xlabel('Predicted')\n",
    "axes[2].set_ylabel('Actual')\n",
    "axes[2].set_xticklabels(['Stay', 'Leave'])\n",
    "axes[2].set_yticklabels(['Stay', 'Leave'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print detailed confusion matrix statistics\n",
    "models = ['Dummy', 'Logistic Regression', 'Random Forest']\n",
    "cms = [dummy_cm, log_reg_cm, rf_cm]\n",
    "\n",
    "for model, cm in zip(models, cms):\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    print(f\"\\n{model}:\")\n",
    "    print(f\"  True Negatives (Stay correctly predicted): {tn}\")\n",
    "    print(f\"  False Positives (Stay predicted as Leave): {fp}\")\n",
    "    print(f\"  False Negatives (Leave predicted as Stay): {fn}\")\n",
    "    print(f\"  True Positives (Leave correctly predicted): {tp}\")\n",
    "    print(f\"  Accuracy: {(tp + tn) / (tp + tn + fp + fn):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1b2239",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T12:57:35.814646Z",
     "start_time": "2025-07-23T12:57:35.804952Z"
    },
    "execution": {
     "iopub.execute_input": "2025-07-23T13:32:19.314247Z",
     "iopub.status.busy": "2025-07-23T13:32:19.314009Z",
     "iopub.status.idle": "2025-07-23T13:32:19.393072Z",
     "shell.execute_reply": "2025-07-23T13:32:19.390835Z"
    }
   },
   "outputs": [],
   "source": [
    "# Detailed classification reports\n",
    "print(\"Classification Reports:\")\n",
    "print(\"=\" * 27)\n",
    "\n",
    "models = ['Dummy Classifier', 'Logistic Regression', 'Random Forest']\n",
    "predictions = [dummy_pred, log_reg_pred, rf_pred]\n",
    "\n",
    "for model, pred in zip(models, predictions):\n",
    "    print(f\"\\n{model}:\")\n",
    "    print(\"=\"*50)\n",
    "    print(classification_report(y_test, pred, target_names=['Stay', 'Leave']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d0392e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T12:57:35.972919Z",
     "start_time": "2025-07-23T12:57:35.852789Z"
    },
    "execution": {
     "iopub.execute_input": "2025-07-23T13:32:19.401815Z",
     "iopub.status.busy": "2025-07-23T13:32:19.401526Z",
     "iopub.status.idle": "2025-07-23T13:32:19.946290Z",
     "shell.execute_reply": "2025-07-23T13:32:19.945786Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# ROC curves\n",
    "print(\"ROC Curves:\")\n",
    "print(\"=\" * 13)\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Logistic Regression ROC\n",
    "fpr_lr, tpr_lr, _ = roc_curve(y_test, log_reg_pred_proba)\n",
    "plt.plot(fpr_lr, tpr_lr, label=f'Logistic Regression (AUC = {log_reg_auc:.3f})', linewidth=2)\n",
    "\n",
    "# Random Forest ROC\n",
    "fpr_rf, tpr_rf, _ = roc_curve(y_test, rf_pred_proba)\n",
    "plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {rf_auc:.3f})', linewidth=2)\n",
    "\n",
    "# Diagonal line (random classifier)\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier (AUC = 0.500)', linewidth=1)\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves - Model Comparison')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Precision-Recall curves\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Logistic Regression PR\n",
    "precision_lr, recall_lr, _ = precision_recall_curve(y_test, log_reg_pred_proba)\n",
    "plt.plot(recall_lr, precision_lr, label=f'Logistic Regression', linewidth=2)\n",
    "\n",
    "# Random Forest PR\n",
    "precision_rf, recall_rf, _ = precision_recall_curve(y_test, rf_pred_proba)\n",
    "plt.plot(recall_rf, precision_rf, label=f'Random Forest', linewidth=2)\n",
    "\n",
    "# Baseline (proportion of positive class)\n",
    "baseline_precision = y_test.mean()\n",
    "plt.axhline(y=baseline_precision, color='k', linestyle='--', label=f'Baseline ({baseline_precision:.3f})', linewidth=1)\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curves - Model Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbe3741",
   "metadata": {},
   "source": [
    "## 5. Model Insights & Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca17c3ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T12:57:36.158190Z",
     "start_time": "2025-07-23T12:57:36.002354Z"
    },
    "execution": {
     "iopub.execute_input": "2025-07-23T13:32:19.949365Z",
     "iopub.status.busy": "2025-07-23T13:32:19.949122Z",
     "iopub.status.idle": "2025-07-23T13:32:20.467449Z",
     "shell.execute_reply": "2025-07-23T13:32:20.466970Z"
    }
   },
   "outputs": [],
   "source": [
    "# Feature importance comparison\n",
    "print(\"Feature Importance Analysis:\")\n",
    "print(\"=\" * 32)\n",
    "\n",
    "# Combine feature importance from both models\n",
    "importance_comparison = pd.DataFrame({\n",
    "    'feature': X_train_final.columns,\n",
    "    'logistic_coef': np.abs(log_reg.coef_[0]),\n",
    "    'rf_importance': rf.feature_importances_\n",
    "})\n",
    "\n",
    "# Normalize importance scores for comparison\n",
    "importance_comparison['logistic_coef_norm'] = importance_comparison['logistic_coef'] / importance_comparison['logistic_coef'].max()\n",
    "importance_comparison['rf_importance_norm'] = importance_comparison['rf_importance'] / importance_comparison['rf_importance'].max()\n",
    "\n",
    "# Sort by random forest importance\n",
    "importance_comparison = importance_comparison.sort_values('rf_importance', ascending=False)\n",
    "\n",
    "print(\"Top 15 features by Random Forest importance:\")\n",
    "print(importance_comparison.head(15)[['feature', 'rf_importance', 'logistic_coef']].to_string(index=False, float_format='%.4f'))\n",
    "\n",
    "# Visualize top features\n",
    "top_features = importance_comparison.head(15)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "\n",
    "# Random Forest importance\n",
    "axes[0].barh(range(len(top_features)), top_features['rf_importance'])\n",
    "axes[0].set_yticks(range(len(top_features)))\n",
    "axes[0].set_yticklabels(top_features['feature'])\n",
    "axes[0].set_xlabel('Random Forest Importance')\n",
    "axes[0].set_title('Top 15 Features - Random Forest')\n",
    "\n",
    "# Logistic Regression coefficients\n",
    "axes[1].barh(range(len(top_features)), top_features['logistic_coef'])\n",
    "axes[1].set_yticks(range(len(top_features)))\n",
    "axes[1].set_yticklabels(top_features['feature'])\n",
    "axes[1].set_xlabel('Absolute Coefficient Value')\n",
    "axes[1].set_title('Top 15 Features - Logistic Regression')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca114864",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T12:57:36.193569Z",
     "start_time": "2025-07-23T12:57:36.189671Z"
    },
    "execution": {
     "iopub.execute_input": "2025-07-23T13:32:20.470058Z",
     "iopub.status.busy": "2025-07-23T13:32:20.469809Z",
     "iopub.status.idle": "2025-07-23T13:32:20.484672Z",
     "shell.execute_reply": "2025-07-23T13:32:20.484185Z"
    }
   },
   "outputs": [],
   "source": [
    "# Overall model evaluation summary\n",
    "print(\"MODEL EVALUATION SUMMARY:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "print(f\"\\n1. DATASET CHARACTERISTICS:\")\n",
    "print(f\"   - Total samples: {len(y)}\")\n",
    "print(f\"   - Training samples: {len(y_train)}\")\n",
    "print(f\"   - Test samples: {len(y_test)}\")\n",
    "print(f\"   - Features: {X_train_final.shape[1]}\")\n",
    "print(f\"   - Class imbalance: {(y == 0).sum()}:{(y == 1).sum()} (Stay:Leave)\")\n",
    "\n",
    "print(f\"\\n2. MODEL PERFORMANCE RANKING (by F1-Score):\")\n",
    "performance_ranking = results_df.sort_values('F1-Score', ascending=False)\n",
    "for i, (_, row) in enumerate(performance_ranking.iterrows()):\n",
    "    print(f\"   {i+1}. {row['Model']}: F1={row['F1-Score']:.4f}, Precision={row['Precision']:.4f}, Recall={row['Recall']:.4f}\")\n",
    "\n",
    "print(f\"\\n3. BEST MODEL ANALYSIS:\")\n",
    "best_model = performance_ranking.iloc[0]\n",
    "print(f\"   - Best performing model: {best_model['Model']}\")\n",
    "print(f\"   - F1-Score: {best_model['F1-Score']:.4f}\")\n",
    "print(f\"   - Precision: {best_model['Precision']:.4f}\")\n",
    "print(f\"   - Recall: {best_model['Recall']:.4f}\")\n",
    "print(f\"   - AUC-ROC: {best_model['AUC-ROC']:.4f}\")\n",
    "\n",
    "print(f\"\\n4. IMPROVEMENT OVER BASELINE:\")\n",
    "baseline_f1 = dummy_results[best_dummy_strategy]['f1']\n",
    "best_f1 = performance_ranking.iloc[0]['F1-Score']\n",
    "improvement = (best_f1 - baseline_f1) / baseline_f1 * 100\n",
    "print(f\"   - Baseline F1-Score: {baseline_f1:.4f}\")\n",
    "print(f\"   - Best F1-Score: {best_f1:.4f}\")\n",
    "print(f\"   - Improvement: {improvement:.1f}%\")\n",
    "\n",
    "print(f\"\\n5. KEY INSIGHTS:\")\n",
    "print(f\"   - Both ML models significantly outperform the dummy baseline\")\n",
    "print(f\"   - Random Forest shows {'better' if rf_f1 > log_reg_f1 else 'similar' if abs(rf_f1 - log_reg_f1) < 0.01 else 'worse'} performance than Logistic Regression\")\n",
    "print(f\"   - AUC-ROC values indicate {'good' if max(rf_auc, log_reg_auc) > 0.8 else 'moderate' if max(rf_auc, log_reg_auc) > 0.7 else 'fair'} discriminative ability\")\n",
    "print(f\"   - Class imbalance may be affecting performance - consider addressing in next phase\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17939219",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-23T12:57:36.274441Z",
     "start_time": "2025-07-23T12:57:36.236811Z"
    },
    "execution": {
     "iopub.execute_input": "2025-07-23T13:32:20.488120Z",
     "iopub.status.busy": "2025-07-23T13:32:20.487268Z",
     "iopub.status.idle": "2025-07-23T13:32:20.565036Z",
     "shell.execute_reply": "2025-07-23T13:32:20.564554Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save baseline results to database\n",
    "from hr_analytics_utils import save_model_results_to_db, save_feature_importance_to_db\n",
    "\n",
    "print(\"SAVING RESULTS TO DATABASE\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Save model comparison results to database\n",
    "save_model_results_to_db(results_df, 'baseline_model_results', \"../results/technova_hr.db\")\n",
    "\n",
    "# Save feature importance to database\n",
    "save_feature_importance_to_db(importance_comparison, 'baseline_feature_importance', \"../results/technova_hr.db\")\n",
    "\n",
    "# Save predictions for further analysis\n",
    "predictions_df = pd.DataFrame({\n",
    "    'actual': y_test,\n",
    "    'dummy_pred': dummy_pred,\n",
    "    'logistic_pred': log_reg_pred,\n",
    "    'rf_pred': rf_pred,\n",
    "    'logistic_proba': log_reg_pred_proba,\n",
    "    'rf_proba': rf_pred_proba\n",
    "})\n",
    "\n",
    "# Save predictions to database\n",
    "save_model_results_to_db(predictions_df, 'baseline_predictions', \"../results/technova_hr.db\")\n",
    "\n",
    "print(f\"\\nAll baseline modeling results saved to database!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "automated-classification-dNv-zZCP-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
